VOLUMES

Let's look at what volumes are meant to be in Docker container. 
Docker container are meant to host only for a short period of time, process the data and then get destroyed. The same of course is true for the data inside the container
that will be destroyed when the container finish his job.

To persist data process by the container we attach a volume to the container when they are created. The data process by the containers is now placed in this volume. Now Even if
the container is deleted the data inside the volume remains.

So how does it work in k8s?
Just like in Docker the Pod are meant to be transient and get destroyed when their job finished, included the data the pod produces. 
For that we attached a Volume to the pod, so even after the pod is deleted the data remains.


Let's consider the following pod

apiVersion: v1
kind: Pod
metadata:
	name: random-number-generator
spec:
	containers:
		- image: alpine
		  name: alpine
		  command: ["/bin/sh","-c"]
		  args: ["shuf -i 0-100 -n 1 >> /opt/number.out;"]

With this pod generate a ramndom number and save it in a file. When the pod is destroyed the file is destroyed too.
To retain the number generated by the pod we create a volume

apiVersion: v1
kind: Pod
metadata:
	name: random-number-generator
spec:
	containers:
		- image: alpine
		  name: alpine
		  command: ["/bin/sh","-c"]
		  args: ["shuf -i 0-100 -n 1 >> /opt/number.out;"]
		  volumeMounts:
			- mountPaht: /opt
			  name: data-volume
	volumes:
		- name: data-volume
		  hostPath:
			# In this case I specify a path from the host
			path: /data
			type: Directory

Now when the pod is deleted the file with the random number generated remains in the host.

If when the pod is started you check the /data folder in your host you will find the number.out file

This works fine for a single Node, but when you have multiple node, things starts being complicated since the hostPath refers to the Node of course.
The pod will use the /data of each node and expect all of them to be the same and have the same data. If you think of a database, you can immediatly be aware that that's not the truth.
If the pod of node 1 update a record, the pod on node 2 does not see it.

---------- ---------- ----------- 		 
|	 | |	    | |		|		
|	 | |	    | |		|		
|	 | |	    | |		|		
|	 | |	    | | 	|		 
| /data	 | | /data  | |	/data 	|		 
| Node 1 | | Node 2 | |	Node 3  |		 
---------- ---------- -----------		 

Unless you configured some cluster replicated solutions, such as:
NFS
GlusterFS
Flocker
Ceph
SCALEIO
AWS EBS (Elastic block store)
Google persistence Disk

For example in AWS you have as the storage


	volumes:
		- name: data-volume
		  awsElasticBlockStore:
			volumeID: <volume-id>
			fsType: ext4

The volume storage will be on AWS EBS and not on a single node